{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecSys-Yt.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwwpfC--_rjl",
        "colab_type": "code",
        "outputId": "082e411f-a310-4bbc-8dfd-6d38679fbe21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "# from keras.optimizers import Adam\n",
        "from keras.optimizers import adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.estimator import ModeKeys\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import csv\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "TRAIN = ModeKeys.TRAIN\n",
        "EVAL = ModeKeys.EVAL"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0623 20:01:48.756881 139695598520192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:10: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQNjy7F8LDn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUGsnoamOukq",
        "colab_type": "code",
        "outputId": "7067162a-82e0-4f4e-d141-03a0adbaf151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('drive/My Drive/recsys')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HuyZa4t_w6O",
        "colab_type": "code",
        "outputId": "0eaa1599-87cc-4609-e420-15a2812118bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "PATH = 'data/'\n",
        "train = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
        "train.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>session_id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>step</th>\n",
              "      <th>action_type</th>\n",
              "      <th>reference</th>\n",
              "      <th>platform</th>\n",
              "      <th>city</th>\n",
              "      <th>device</th>\n",
              "      <th>current_filters</th>\n",
              "      <th>impressions</th>\n",
              "      <th>prices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00RL8Z82B2Z1</td>\n",
              "      <td>aff3928535f48</td>\n",
              "      <td>1541037460</td>\n",
              "      <td>1</td>\n",
              "      <td>search for poi</td>\n",
              "      <td>Newtown</td>\n",
              "      <td>AU</td>\n",
              "      <td>Sydney, Australia</td>\n",
              "      <td>mobile</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00RL8Z82B2Z1</td>\n",
              "      <td>aff3928535f48</td>\n",
              "      <td>1541037522</td>\n",
              "      <td>2</td>\n",
              "      <td>interaction item image</td>\n",
              "      <td>666856</td>\n",
              "      <td>AU</td>\n",
              "      <td>Sydney, Australia</td>\n",
              "      <td>mobile</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00RL8Z82B2Z1</td>\n",
              "      <td>aff3928535f48</td>\n",
              "      <td>1541037522</td>\n",
              "      <td>3</td>\n",
              "      <td>interaction item image</td>\n",
              "      <td>666856</td>\n",
              "      <td>AU</td>\n",
              "      <td>Sydney, Australia</td>\n",
              "      <td>mobile</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00RL8Z82B2Z1</td>\n",
              "      <td>aff3928535f48</td>\n",
              "      <td>1541037532</td>\n",
              "      <td>4</td>\n",
              "      <td>interaction item image</td>\n",
              "      <td>666856</td>\n",
              "      <td>AU</td>\n",
              "      <td>Sydney, Australia</td>\n",
              "      <td>mobile</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00RL8Z82B2Z1</td>\n",
              "      <td>aff3928535f48</td>\n",
              "      <td>1541037532</td>\n",
              "      <td>5</td>\n",
              "      <td>interaction item image</td>\n",
              "      <td>109038</td>\n",
              "      <td>AU</td>\n",
              "      <td>Sydney, Australia</td>\n",
              "      <td>mobile</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        user_id     session_id   timestamp  ...  current_filters impressions prices\n",
              "0  00RL8Z82B2Z1  aff3928535f48  1541037460  ...              NaN         NaN    NaN\n",
              "1  00RL8Z82B2Z1  aff3928535f48  1541037522  ...              NaN         NaN    NaN\n",
              "2  00RL8Z82B2Z1  aff3928535f48  1541037522  ...              NaN         NaN    NaN\n",
              "3  00RL8Z82B2Z1  aff3928535f48  1541037532  ...              NaN         NaN    NaN\n",
              "4  00RL8Z82B2Z1  aff3928535f48  1541037532  ...              NaN         NaN    NaN\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFM4WAuzFS_1",
        "colab_type": "code",
        "outputId": "87c2513d-5a1b-4aa1-ed7b-ba6ac7871a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "item_metadata = pd.read_csv(os.path.join(PATH, 'item_metadata.csv'))\n",
        "item_metadata.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>item_id</th>\n",
              "      <th>properties</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5101</td>\n",
              "      <td>Satellite TV|Golf Course|Airport Shuttle|Cosme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5416</td>\n",
              "      <td>Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5834</td>\n",
              "      <td>Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5910</td>\n",
              "      <td>Satellite TV|Sailing|Cosmetic Mirror|Telephone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6066</td>\n",
              "      <td>Satellite TV|Sailing|Diving|Cosmetic Mirror|Sa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   item_id                                         properties\n",
              "0     5101  Satellite TV|Golf Course|Airport Shuttle|Cosme...\n",
              "1     5416  Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...\n",
              "2     5834  Satellite TV|Cosmetic Mirror|Safe (Hotel)|Tele...\n",
              "3     5910  Satellite TV|Sailing|Cosmetic Mirror|Telephone...\n",
              "4     6066  Satellite TV|Sailing|Diving|Cosmetic Mirror|Sa..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24LoBTiIoAlZ",
        "colab_type": "code",
        "outputId": "92a3ba54-a9eb-49c0-ae39-0433d5266f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "item_metadata['item_id'].hist()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0ce89a6630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEJCAYAAABohnsfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSepmaEiSuv1/LqZ7Y6sIRk8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkcZjE0ZHSgE",
        "colab_type": "text"
      },
      "source": [
        "## Extracting filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8VxoK8WEGDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Treating them as independent... which is definitely not correct\n",
        "# but anyway\n",
        "filters_statistics = defaultdict(int)\n",
        "\n",
        "\n",
        "for filters in train['current_filters'].dropna():\n",
        "  for current_filter in filters.split('|'):\n",
        "    filters_statistics[current_filter] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6wOXY6SITsg",
        "colab_type": "code",
        "outputId": "2a9866d1-ffdc-47b7-b71e-8997e9c1453d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "filters = list(filters_statistics.keys())\n",
        "filters[:10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Very Good Rating',\n",
              " '5 Star',\n",
              " '4 Star',\n",
              " 'Hotel',\n",
              " 'Motel',\n",
              " 'Resort',\n",
              " 'Hostal (ES)',\n",
              " '3 Star',\n",
              " 'Best Value',\n",
              " 'Free WiFi (Combined)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REpkQEySplzG",
        "colab_type": "text"
      },
      "source": [
        "## Creating tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfjvkpMGpnvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tokenizer:\n",
        "  \"\"\"\n",
        "  Mapping ids to ints.\n",
        "  \"\"\"\n",
        "  \n",
        "  def fit(self, data):\n",
        "    self.vocabulary = set(data)\n",
        "    self.mapping = dict(zip(self.vocabulary, range(1, len(self.vocabulary) + 1)))\n",
        "    return self\n",
        "    \n",
        "  def transform(self, data):\n",
        "    if isinstance(data, list):\n",
        "      return [self.mapping.get(d, 0) for d in data]\n",
        "    else:\n",
        "      return self.mapping.get(data, 0) \n",
        "\n",
        "item_tokenizer = Tokenizer().fit(item_metadata['item_id'])\n",
        "user_tokenizer = Tokenizer().fit(train['user_id'])\n",
        "session_tokenizer = Tokenizer().fit(train['session_id'])\n",
        "\n",
        "action_type_tokenizer = Tokenizer().fit(train['action_type'])\n",
        "platform_tokenizer = Tokenizer().fit(train['platform'])\n",
        "city_tokenizer = Tokenizer().fit(train['city'])\n",
        "device_tokenizer = Tokenizer().fit(train['device'])\n",
        "filters_tokenizer = Tokenizer().fit(filters)\n",
        "\n",
        "tokenizers = {\n",
        "  'item_id': item_tokenizer,\n",
        "  'user_id': user_tokenizer,\n",
        "  'session_id': session_tokenizer,\n",
        "  'action_type': action_type_tokenizer,\n",
        "  'platform': platform_tokenizer,\n",
        "  'city': city_tokenizer,\n",
        "  'device': device_tokenizer,\n",
        "  'filters': filters_tokenizer\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQnXyzXv3Rf6",
        "colab_type": "code",
        "outputId": "e17ec585-fb84-43c5-e2d2-3eea5df5f9ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "user_tokenizer.transform(['00RL8Z82B2Z1'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[345341]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah63Ucm6da_7",
        "colab_type": "code",
        "outputId": "52bd7b6a-d8b9-4552-94e2-07cdd47bb4e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "item_tokenizer.transform(666856)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "339660"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ5YrsTPVJ-9",
        "colab_type": "code",
        "outputId": "ab1afa66-e2e9-4d24-8e47-5eea92637415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(platform_tokenizer.vocabulary)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1juJ9MuVZ1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are actions that involve a particular item (except for clickout item)\n",
        "ITEM_ACTIONS = [\n",
        " #'interaction item deals',\n",
        " 'interaction item image',\n",
        " #'interaction item info',\n",
        " #'interaction item rating',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al5yGpdbg2ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_length = int(len(train) * 0.8)\n",
        "train_split = train[:train_length]\n",
        "val_split = train[train_length:]\n",
        "len(train_split), len(val_split)\n",
        "del train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2CeQkwcCRjW",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ2YOhwqQvy7",
        "colab_type": "text"
      },
      "source": [
        "В момента, ако имаме сесия с няколко clickouts: (A, B, C):\n",
        "Начало ....... A.......B.....C........\n",
        "Генераторът връща\n",
        "- данните от началото до А\n",
        "- данните от началото до B (с А)\n",
        "- данните от началото до C (с A и B)\n",
        "С ИЗКЛЮЧЕНИЕ, че след всеки clickout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdO234mYB9gY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputGenerator:\n",
        "  all_fields = ['user_id', 'session_id', 'platform', 'city',\n",
        "                'device','action_types', 'interacted_items',\n",
        "                'filters', 'items', 'prices', 'timestamp']\n",
        "  simple_fields = ['user_id', 'session_id', 'platform', 'city', 'device']\n",
        "  \n",
        "  def __init__(self, tokenizers, returned_fields=None):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.returned_fields = returned_fields\n",
        "    print(self.returned_fields)\n",
        "    num_fields = len(returned_fields) if returned_fields else len(self.all_fields)\n",
        "    self.output_types = tuple((tf.int32 for _ in range(num_fields)))\n",
        "    self.padded_shapes = tuple([tf.TensorShape([None]) for _ in range(num_fields)])\n",
        "    \n",
        "    \n",
        "  def prepare_last_row(self, row): \n",
        "    # Dealing with current_filters\timpressions\tprices\n",
        "    impressions = list(map(int, row['impressions'].split('|')))\n",
        "    items = self.tokenizers['item_id'].transform(impressions)\n",
        "    prices = list(map(int, row['prices'].split('|')))\n",
        "    clicked_item = self.tokenizers['item_id'].transform(int(row['reference']))\n",
        "\n",
        "    return items, prices, clicked_item\n",
        "\n",
        "  def input_generator_gen(self, df):\n",
        "    def gen():\n",
        "      last_step = None\n",
        "      session_id = None\n",
        "      \n",
        "      for index, row in df.iterrows():\n",
        "        user_id, new_session_id, platform, city, device = [self.tokenizers[field].transform(row[field]) for field in self.simple_fields]\n",
        "\n",
        "        if session_id != new_session_id:\n",
        "          # Reinitialize everything because a new session started.\n",
        "          session_id = new_session_id\n",
        "          action_types, interacted_items = [], []\n",
        "          filters = set()\n",
        "          last_step = 0\n",
        "       \n",
        "        last_step += 1\n",
        "        # assert row['step'] == last_step\n",
        "        \n",
        "        if not pd.isnull(row['current_filters']):\n",
        "          filters.update(self.tokenizers['filters'].transform(row['current_filters'].split('|')))\n",
        "\n",
        "        if row['action_type'] in ITEM_ACTIONS and not pd.isnull(row['reference']):\n",
        "          interacted_item = self.tokenizers['item_id'].transform(int(row['reference']))\n",
        "          interacted_items.append(interacted_item)\n",
        "        \n",
        "        action_types.append(self.tokenizers['action_type'].transform(row['action_type']))\n",
        "\n",
        "        # Checking if it is the last row from the session\n",
        "        if row['action_type'] == 'clickout item':\n",
        "          new_session = True\n",
        "          items, prices, clicked_item = self.prepare_last_row(row)\n",
        "          result = {\n",
        "              'user_id': [user_id],\n",
        "              'session_id': [session_id],\n",
        "              'platform': [platform],\n",
        "              'city': [city],\n",
        "              'device': [device],\n",
        "              'action_types': action_types,\n",
        "              'interacted_items': interacted_items,\n",
        "              'filters': list(filters),\n",
        "              'items': items,\n",
        "              'prices': prices,\n",
        "              'timestamp': [row['timestamp']],\n",
        "          }\n",
        "          \n",
        "          if self.returned_fields:\n",
        "            returned_values = [result[f] for f in self.returned_fields]\n",
        "          else:\n",
        "            returned_values = result.values()\n",
        "          \n",
        "          yield tuple(returned_values), [clicked_item]\n",
        "          interacted_items = []\n",
        "\n",
        "\n",
        "    return gen, (self.output_types, tf.int32), (self.padded_shapes, tf.TensorShape([None]))\n",
        "  \n",
        "# input_user, input_platform, input_device, input_filters,\n",
        "# input_interacted_items, input_impression_items\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57Gak2iQvVYT",
        "colab_type": "code",
        "outputId": "e30c39af-48bc-4614-beec-0748f5eef264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "i = InputGenerator(tokenizers,\n",
        "                   returned_fields=['user_id', 'session_id', 'interacted_items'])\n",
        "gen, types, shapes = i.input_generator_gen(train)\n",
        "dataset = tf.data.Dataset.from_generator(gen, types)\n",
        "dataset = dataset.take(20)\n",
        "dataset = dataset.repeat().padded_batch(2, padded_shapes=shapes)\n",
        "batch = dataset.make_one_shot_iterator().get_next()\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run([tf.local_variables_initializer(), tf.tables_initializer()])\n",
        "  b = sess.run(batch)\n",
        "b\n",
        "\"\"\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ni = InputGenerator(tokenizers,\\n                   returned_fields=['user_id', 'session_id', 'interacted_items'])\\ngen, types, shapes = i.input_generator_gen(train)\\ndataset = tf.data.Dataset.from_generator(gen, types)\\ndataset = dataset.take(20)\\ndataset = dataset.repeat().padded_batch(2, padded_shapes=shapes)\\nbatch = dataset.make_one_shot_iterator().get_next()\\n\\n\\nwith tf.Session() as sess:\\n  sess.run([tf.local_variables_initializer(), tf.tables_initializer()])\\n  b = sess.run(batch)\\nb\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHJdHiskn8Gy",
        "colab_type": "text"
      },
      "source": [
        "Малко промяна в модела:\n",
        "\n",
        "\n",
        "*   по време на трениране, учим embedding на потребителя, който ще отразява цялата му активност, НО average на embeddings на items не правим върху всички такива, които е купувал някога (както в youtube са всики такива, които е гледал някога), а на тези, които са в impressions - мотивацията ми е, че тези items 1. приличат на target click_out item-a и 2. отговарят на действията/филтрите в сесията.\n",
        "*   пак е един вид bag of actions, защото не ни интересува реда на actions, на сесии и тн., но просто сме групирали данните по сесии, учим по сесии.\n",
        "\n",
        "input layer:\n",
        "*   user embedding\n",
        "*   ~session embedding~\n",
        "*  ~action types~ \n",
        "* average of embeddings of the interacted with items (keeping the distribution! If the user has interacted twice with an item - summing it twice)\n",
        "* average of embedding of the impression items\n",
        "* filters - one-hot encoded, 1s at the used filters\n",
        "*  platform - one-hot encoded\n",
        "*  devices - one-hot encoded (is it worth it? are they even relevant...)\n",
        "* any idea how to include the prices??? are they necessary? they are a property of the items in the end..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFKNt5hVl3Rg",
        "colab_type": "code",
        "outputId": "772dfa04-56b1-4a52-fcf3-a2484d4326bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"# TODO: work it out someway... BUG!\n",
        "# https://github.com/keras-team/keras/issues/10472\n",
        "class ReciprocalRank(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, dataset_val, batch_size = 20):\n",
        "        super().__init__()\n",
        "        it = dataset_val.make_one_shot_iterator().get_next()\n",
        "        self.val_input_batch, self.val_target_items_batch = it\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self._data = []\n",
        "        self.rrs = []\n",
        "\n",
        "    def get_data(self):\n",
        "        return self._data\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        predict = np.asarray(self.model.predict(x=self.val_input_batch,\n",
        "                                                batch_size=self.batch_size,\n",
        "                                                steps=1))\n",
        "        #targ = sel.model.validation_data[1]\n",
        "        self._data.append(predict)\n",
        "        \n",
        "custom_metrics = ReciprocalRank()\"\"\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# TODO: work it out someway... BUG!\\n# https://github.com/keras-team/keras/issues/10472\\nclass ReciprocalRank(tf.keras.callbacks.Callback):\\n    \\n    def __init__(self, dataset_val, batch_size = 20):\\n        super().__init__()\\n        it = dataset_val.make_one_shot_iterator().get_next()\\n        self.val_input_batch, self.val_target_items_batch = it\\n        self.batch_size = batch_size\\n\\n    def on_train_begin(self, logs={}):\\n        self._data = []\\n        self.rrs = []\\n\\n    def get_data(self):\\n        return self._data\\n\\n    def on_epoch_end(self, batch, logs={}):\\n        predict = np.asarray(self.model.predict(x=self.val_input_batch,\\n                                                batch_size=self.batch_size,\\n                                                steps=1))\\n        #targ = sel.model.validation_data[1]\\n        self._data.append(predict)\\n        \\ncustom_metrics = ReciprocalRank()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ8TwaqlM6Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReciprocalRankVal(tf.keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, dataset_val, batch_size = 20):\n",
        "        super().__init__()\n",
        "        it = dataset_val.make_one_shot_iterator().get_next()\n",
        "        self.val_input_batch, self.val_target_items_batch = it\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self._data = []\n",
        "        self.rrs = []\n",
        "\n",
        "    def get_data(self):\n",
        "        return self._data\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        predict = np.asarray(self.model.predict(x=self.val_input_batch,\n",
        "                                                batch_size=self.batch_size,\n",
        "                                                steps=1))\n",
        "        #targ = sel.model.validation_data[1]\n",
        "        self._data.append(predict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq8-FtVeBCpZ",
        "colab_type": "code",
        "outputId": "b36adadc-6c56-48b5-96e7-2ea24dd03781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "params = {\n",
        "    'users_embedding': 50,\n",
        "    'users_vocab': len(tokenizers['user_id'].vocabulary),\n",
        "    'items_embedding': 50,\n",
        "    'items_vocab': len(tokenizers['item_id'].vocabulary),\n",
        "    'platforms_vocab': len(tokenizers['platform'].vocabulary),\n",
        "    'devices_vocab': len(tokenizers['device'].vocabulary),\n",
        "    'filters_vocab': len(tokenizers['filters'].vocabulary),\n",
        "    'layer_1_units': 400,\n",
        "    'layer_2_units': 200,\n",
        "    'layer_3_units': 100,\n",
        "    'batch_size': 128\n",
        "    \n",
        "}\n",
        "params"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 128,\n",
              " 'devices_vocab': 3,\n",
              " 'filters_vocab': 202,\n",
              " 'items_embedding': 50,\n",
              " 'items_vocab': 927142,\n",
              " 'layer_1_units': 400,\n",
              " 'layer_2_units': 200,\n",
              " 'layer_3_units': 100,\n",
              " 'platforms_vocab': 55,\n",
              " 'users_embedding': 50,\n",
              " 'users_vocab': 730803}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBCxwfI3ua9e",
        "colab_type": "text"
      },
      "source": [
        "## Train and val datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEMQkApNuYHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e46b1ef-08be-4652-af3d-61e054cf6a89"
      },
      "source": [
        "# DATASET\n",
        "batch_size = params['batch_size']\n",
        "i = InputGenerator(tokenizers,\n",
        "                  ['user_id', 'session_id', 'platform', 'device',\n",
        "                   'interacted_items', 'filters', 'items', 'timestamp'])\n",
        "\n",
        "def get_dataset_train():\n",
        "  gen, types, shapes = i.input_generator_gen(train_split)\n",
        "  dataset_train = tf.data.Dataset.from_generator(gen, types)\n",
        "  dataset_train = dataset_train.repeat().padded_batch(batch_size,\n",
        "                                                      padded_shapes=shapes)\n",
        "  batch = dataset_train.make_one_shot_iterator().get_next()\n",
        "  \n",
        "  return batch\n",
        "\n",
        "def get_dataset_eval():\n",
        "  gen_val, types, shapes = i.input_generator_gen(val_split)\n",
        "  dataset_val = tf.data.Dataset.from_generator(gen_val, types)\n",
        "  dataset_val = dataset_val.padded_batch(batch_size,\n",
        "                                         padded_shapes=shapes)\n",
        "  batch_val = dataset_val.make_one_shot_iterator().get_next()\n",
        "  \n",
        "  return batch_val\n",
        "\n",
        "\n",
        "steps_per_epoch = len(train_split) // batch_size\n",
        "validation_steps = len(val_split) // batch_size\n",
        "# These are not epochs but steps since we're using tf.dataset.\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['user_id', 'session_id', 'platform', 'device', 'interacted_items', 'filters', 'items', 'timestamp']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fejW0O6rub-9",
        "colab_type": "text"
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz8wmWab7gG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGgR47XH3QKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lambda wrappers\n",
        "def expand_dims_axis(axis):\n",
        "  def expand_dims(x):\n",
        "    return tf.expand_dims(x, axis=axis)\n",
        "  return expand_dims\n",
        "\n",
        "def reduce_sum_axis(axis):\n",
        "  def reduce_sum(x):\n",
        "    return tf.reduce_sum(x, axis)\n",
        "  return reduce_sum\n",
        "\n",
        "\n",
        "def yt_rec_model_fn(\n",
        "   features, # This is batch_features from input_fn\n",
        "   labels,   # This is batch_labels from input_fn\n",
        "   mode,     # An instance of tf.estimator.ModeKeys, see below\n",
        "   params):  # Additional configuration\n",
        "\n",
        "  batch_size = params['batch_size'] if mode in [TRAIN, EVAL] else 1\n",
        "\n",
        "  # Input layers\n",
        "\n",
        "  input_user, input_session, input_platform, input_device, \\\n",
        "  input_filters, input_interacted_items, \\\n",
        "  input_impression_items, input_timestamp = features\n",
        "\n",
        "\n",
        "  users_embeddings = layers.Embedding(input_dim=params['users_vocab'],\n",
        "                                      output_dim=params['users_embedding'])\n",
        "  items_embeddings = layers.Embedding(input_dim=params['items_vocab'],\n",
        "                                      output_dim=params['items_embedding'])\n",
        "\n",
        "  input_user.set_shape([None, 1])\n",
        "  input_platform.set_shape([None, 1])\n",
        "  input_device.set_shape([None, 1])\n",
        "  input_interacted_items.set_shape([batch_size, None])\n",
        "  input_impression_items.set_shape([batch_size, None])\n",
        "\n",
        "  user = users_embeddings(input_user)\n",
        "  user.set_shape([None, 1, params['users_embedding']])\n",
        "  user = tf.reshape(user, [-1, params['users_embedding']])\n",
        "\n",
        "  platform = tf.one_hot(input_platform, params['platforms_vocab'])\n",
        "  platform.set_shape([None, 1, params['platforms_vocab']])\n",
        "  platform = tf.reshape(platform, [-1, params['platforms_vocab']])\n",
        "  \n",
        "  device = tf.one_hot(input_device, params['devices_vocab'])\n",
        "  device.set_shape([None, 1, params['devices_vocab']])\n",
        "  device = tf.reshape(device, [-1, params['devices_vocab']])\n",
        "  # Summing them - taking into account all of the filters used.\n",
        "  # TODO: FIX FILTERS\n",
        "  # filters = tf.reduce_sum(tf.one_hot(input_filters, params['filters_vocab']), axis=0)\n",
        "\n",
        "  interacted_items = items_embeddings(input_interacted_items)\n",
        "  interacted_items.set_shape([None, None, params['items_embedding']])\n",
        "  interacted_items = tf.reduce_mean(interacted_items, axis=1)\n",
        "  \n",
        "  impression_items = items_embeddings(input_impression_items)\n",
        "  impression_items.set_shape([None, None, params['items_embedding']])\n",
        "  impression_items = tf.reduce_mean(items_embeddings(input_impression_items), axis=1)\n",
        "\n",
        "\n",
        "  whole_size = params['users_embedding'] + params['platforms_vocab'] + params['devices_vocab'] + 2 * params['items_embedding'] # +  params['filters_vocab']\n",
        "  # 100 + 50 + 55 + 3 + 202\n",
        "\n",
        "  tf.logging.info(whole_size)\n",
        "\n",
        "  # Dense layers\n",
        "  input_layer = tf.concat([user, platform, device, # filters,\n",
        "                          interacted_items, impression_items],\n",
        "                          axis=1)\n",
        "  #input_layer.set_shape([None, whole_size]) \n",
        "  first_layer = layers.Dense(units=params['layer_1_units'],\n",
        "                             activation=tf.keras.activations.relu)(input_layer)\n",
        "  second_layer = layers.Dense(units=params['layer_2_units'],\n",
        "                              activation=tf.keras.activations.relu)(first_layer)\n",
        "  third_layer = layers.Dense(units=params['layer_3_units'],\n",
        "                             activation=tf.keras.activations.relu)(second_layer)\n",
        "  \n",
        "  last_layer_w = W1 = tf.Variable(tf.random_normal([params['layer_3_units'],\n",
        "                                                    params['items_vocab']]))\n",
        "  last_layer_b = tf.Variable(tf.random_normal([params['items_vocab']]))\n",
        "  \n",
        "  \n",
        "  logits = tf.matmul(third_layer, last_layer_w) + last_layer_b\n",
        "\n",
        "  tf.logging.info('HELLO')\n",
        "  if mode in [tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL]:\n",
        "      # Calculate Loss (for both TRAIN and EVAL modes)\n",
        "    # I guess the samples not enough to train the model...\n",
        "    # The loss does not change.\n",
        "    #loss = tf.nn.sampled_softmax_loss(labels=labels,\n",
        "    #                                  inputs=third_layer,\n",
        "    #                                  weights=tf.transpose(last_layer_w),\n",
        "    #                                  biases=last_layer_b,\n",
        "    #                                  num_sampled=90_000, # 10%\n",
        "    #                                  num_classes=params['items_vocab'])\n",
        "    \n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,\n",
        "                                          logits=logits)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "\n",
        "  \n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    tf.summary.scalar('loss-train', tf.reduce_sum(loss))\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "    train_op = optimizer.minimize(\n",
        "        loss=loss,\n",
        "        global_step=tf.train.get_global_step())\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "  \n",
        "  mask_impressions = tf.reduce_sum(tf.one_hot(input_impression_items, params['items_vocab']), axis=1)\n",
        "  #predictions = tf.multiply(mask_impressions, last_layer)\n",
        "  predictions = layers.Softmax()(logits)\n",
        "  predicted_classes = tf.argmax(predictions, 1)\n",
        "\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    predictions = {\n",
        "        'class_ids': predicted_classes,\n",
        "        'probabilities': predictions,\n",
        "        'user_id': input_user,\n",
        "        'session_id': input_session,\n",
        "        'timestamp': input_timestamp\n",
        "    }\n",
        "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "\n",
        "  # Add evaluation metrics (for EVAL mode)\n",
        "  eval_metric_ops = {\n",
        "      \"accuracy\": tf.metrics.accuracy(\n",
        "          labels=labels, predictions=predicted_classes)}\n",
        "  return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf6EwE6I7Ran",
        "colab_type": "code",
        "outputId": "319a9cf5-fa37-42fc-c798-de41e34cfa71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2893
        }
      },
      "source": [
        "run_config = tf.estimator.RunConfig(log_step_count_steps=30,\n",
        "                                    # save_summary_steps=20,\n",
        "                                    save_checkpoints_steps=10000,\n",
        "                                    keep_checkpoint_max=1)\n",
        "\n",
        "yt_estimator = tf.estimator.Estimator(\n",
        "  model_fn=yt_rec_model_fn,\n",
        "  model_dir='./best_model_yt',\n",
        "  params=params,\n",
        "  config=run_config\n",
        ")\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(input_fn=get_dataset_train,\n",
        "                                    max_steps=1 * steps_per_epoch)\n",
        "# throttle_secs=10 lol\n",
        "eval_spec = tf.estimator.EvalSpec(input_fn=get_dataset_eval,\n",
        "                                  throttle_secs=30)\n",
        "\n",
        "tf.estimator.train_and_evaluate(yt_estimator, train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0623 20:18:16.660300 139695598520192 estimator.py:209] Using config: {'_model_dir': './best_model_yt', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 10000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 30, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0cbe10f6a0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0623 20:18:16.663259 139695598520192 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "I0623 20:18:16.665367 139695598520192 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "I0623 20:18:16.668513 139695598520192 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10000 or save_checkpoints_secs None.\n",
            "I0623 20:18:16.807076 139695598520192 estimator.py:1145] Calling model_fn.\n",
            "I0623 20:18:16.847015 139695598520192 <ipython-input-28-4960fb333ea2>:63] 208\n",
            "I0623 20:18:16.911071 139695598520192 <ipython-input-28-4960fb333ea2>:84] HELLO\n",
            "I0623 20:18:17.572159 139695598520192 estimator.py:1147] Done calling model_fn.\n",
            "I0623 20:18:17.574993 139695598520192 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0623 20:18:17.703564 139695598520192 monitored_session.py:240] Graph was finalized.\n",
            "I0623 20:18:17.725248 139695598520192 saver.py:1280] Restoring parameters from ./best_model_yt/model.ckpt-0\n",
            "W0623 20:18:29.783176 139695598520192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1066: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0623 20:18:29.952733 139695598520192 session_manager.py:500] Running local_init_op.\n",
            "I0623 20:18:29.967959 139695598520192 session_manager.py:502] Done running local_init_op.\n",
            "I0623 20:18:30.367657 139695598520192 basic_session_run_hooks.py:606] Saving checkpoints for 0 into ./best_model_yt/model.ckpt.\n",
            "I0623 20:18:49.846749 139695598520192 basic_session_run_hooks.py:262] loss = 14.314441, step = 0\n",
            "I0623 20:19:01.912422 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.48558\n",
            "I0623 20:19:01.914116 139695598520192 basic_session_run_hooks.py:260] loss = 14.278741, step = 30 (12.071 sec)\n",
            "I0623 20:19:14.352774 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.4115\n",
            "I0623 20:19:14.354232 139695598520192 basic_session_run_hooks.py:260] loss = 14.232, step = 60 (12.440 sec)\n",
            "I0623 20:19:25.985989 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.57882\n",
            "I0623 20:19:25.987578 139695598520192 basic_session_run_hooks.py:260] loss = 14.214939, step = 90 (11.633 sec)\n",
            "I0623 20:19:37.639264 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.57438\n",
            "I0623 20:19:37.640990 139695598520192 basic_session_run_hooks.py:260] loss = 14.310204, step = 120 (11.653 sec)\n",
            "I0623 20:19:49.190605 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.5971\n",
            "I0623 20:19:49.192442 139695598520192 basic_session_run_hooks.py:260] loss = 14.16989, step = 150 (11.551 sec)\n",
            "I0623 20:20:01.949161 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.35136\n",
            "I0623 20:20:01.950477 139695598520192 basic_session_run_hooks.py:260] loss = 14.029686, step = 180 (12.758 sec)\n",
            "I0623 20:20:13.636615 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.56685\n",
            "I0623 20:20:13.637959 139695598520192 basic_session_run_hooks.py:260] loss = 13.846344, step = 210 (11.687 sec)\n",
            "I0623 20:20:25.694929 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.48792\n",
            "I0623 20:20:25.696439 139695598520192 basic_session_run_hooks.py:260] loss = 13.624563, step = 240 (12.058 sec)\n",
            "I0623 20:20:37.122328 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.62526\n",
            "I0623 20:20:37.123643 139695598520192 basic_session_run_hooks.py:260] loss = 13.751141, step = 270 (11.427 sec)\n",
            "I0623 20:20:48.892293 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.54886\n",
            "I0623 20:20:48.897254 139695598520192 basic_session_run_hooks.py:260] loss = 13.938082, step = 300 (11.774 sec)\n",
            "I0623 20:21:00.687840 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.54333\n",
            "I0623 20:21:00.689008 139695598520192 basic_session_run_hooks.py:260] loss = 13.586237, step = 330 (11.792 sec)\n",
            "I0623 20:21:12.610783 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.51616\n",
            "I0623 20:21:12.612143 139695598520192 basic_session_run_hooks.py:260] loss = 13.916146, step = 360 (11.923 sec)\n",
            "I0623 20:21:23.935412 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.64909\n",
            "I0623 20:21:23.936731 139695598520192 basic_session_run_hooks.py:260] loss = 13.5782795, step = 390 (11.325 sec)\n",
            "I0623 20:21:35.624708 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.56646\n",
            "I0623 20:21:35.626803 139695598520192 basic_session_run_hooks.py:260] loss = 13.534817, step = 420 (11.690 sec)\n",
            "I0623 20:21:46.334239 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.80124\n",
            "I0623 20:21:46.335479 139695598520192 basic_session_run_hooks.py:260] loss = 13.399771, step = 450 (10.709 sec)\n",
            "I0623 20:21:57.955465 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.58148\n",
            "I0623 20:21:57.956773 139695598520192 basic_session_run_hooks.py:260] loss = 13.4997835, step = 480 (11.621 sec)\n",
            "I0623 20:22:09.094763 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.69317\n",
            "I0623 20:22:09.096092 139695598520192 basic_session_run_hooks.py:260] loss = 13.246192, step = 510 (11.139 sec)\n",
            "I0623 20:22:20.725762 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.57932\n",
            "I0623 20:22:20.727334 139695598520192 basic_session_run_hooks.py:260] loss = 13.697268, step = 540 (11.631 sec)\n",
            "I0623 20:22:32.321967 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.58706\n",
            "I0623 20:22:32.323290 139695598520192 basic_session_run_hooks.py:260] loss = 13.689092, step = 570 (11.596 sec)\n",
            "I0623 20:22:43.437362 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.69895\n",
            "I0623 20:22:43.439316 139695598520192 basic_session_run_hooks.py:260] loss = 13.162459, step = 600 (11.116 sec)\n",
            "I0623 20:22:55.595776 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.46743\n",
            "I0623 20:22:55.596863 139695598520192 basic_session_run_hooks.py:260] loss = 13.56782, step = 630 (12.158 sec)\n",
            "I0623 20:23:07.608742 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.4973\n",
            "I0623 20:23:07.610079 139695598520192 basic_session_run_hooks.py:260] loss = 14.025494, step = 660 (12.013 sec)\n",
            "I0623 20:23:19.818947 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.45697\n",
            "I0623 20:23:19.820279 139695598520192 basic_session_run_hooks.py:260] loss = 13.0447235, step = 690 (12.210 sec)\n",
            "I0623 20:23:31.118971 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.65487\n",
            "I0623 20:23:31.120372 139695598520192 basic_session_run_hooks.py:260] loss = 13.858433, step = 720 (11.300 sec)\n",
            "I0623 20:23:41.974514 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.76356\n",
            "I0623 20:23:41.976490 139695598520192 basic_session_run_hooks.py:260] loss = 13.349245, step = 750 (10.856 sec)\n",
            "I0623 20:23:53.592783 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.58213\n",
            "I0623 20:23:53.594169 139695598520192 basic_session_run_hooks.py:260] loss = 13.747896, step = 780 (11.618 sec)\n",
            "I0623 20:24:06.401195 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.34221\n",
            "I0623 20:24:06.402546 139695598520192 basic_session_run_hooks.py:260] loss = 13.302227, step = 810 (12.808 sec)\n",
            "I0623 20:24:17.734599 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.64704\n",
            "I0623 20:24:17.735941 139695598520192 basic_session_run_hooks.py:260] loss = 13.775533, step = 840 (11.333 sec)\n",
            "I0623 20:24:29.792379 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.48802\n",
            "I0623 20:24:29.793662 139695598520192 basic_session_run_hooks.py:260] loss = 13.99683, step = 870 (12.058 sec)\n",
            "I0623 20:24:41.720327 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.5151\n",
            "I0623 20:24:41.723547 139695598520192 basic_session_run_hooks.py:260] loss = 13.822334, step = 900 (11.930 sec)\n",
            "I0623 20:24:52.921076 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.67839\n",
            "I0623 20:24:52.922417 139695598520192 basic_session_run_hooks.py:260] loss = 13.508531, step = 930 (11.199 sec)\n",
            "I0623 20:25:04.484236 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.59445\n",
            "I0623 20:25:04.485530 139695598520192 basic_session_run_hooks.py:260] loss = 12.798315, step = 960 (11.563 sec)\n",
            "I0623 20:25:15.847025 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.6402\n",
            "I0623 20:25:15.848525 139695598520192 basic_session_run_hooks.py:260] loss = 13.6960945, step = 990 (11.363 sec)\n",
            "I0623 20:25:27.299465 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.61953\n",
            "I0623 20:25:27.300807 139695598520192 basic_session_run_hooks.py:260] loss = 13.786207, step = 1020 (11.452 sec)\n",
            "I0623 20:25:38.723540 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.62604\n",
            "I0623 20:25:38.724912 139695598520192 basic_session_run_hooks.py:260] loss = 13.544619, step = 1050 (11.424 sec)\n",
            "I0623 20:25:50.456298 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.55694\n",
            "I0623 20:25:50.458197 139695598520192 basic_session_run_hooks.py:260] loss = 13.561796, step = 1080 (11.733 sec)\n",
            "I0623 20:26:01.817377 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.6406\n",
            "I0623 20:26:01.818658 139695598520192 basic_session_run_hooks.py:260] loss = 13.458639, step = 1110 (11.360 sec)\n",
            "I0623 20:26:13.028089 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.67601\n",
            "I0623 20:26:13.029403 139695598520192 basic_session_run_hooks.py:260] loss = 13.914122, step = 1140 (11.211 sec)\n",
            "I0623 20:26:24.421326 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.63314\n",
            "I0623 20:26:24.422612 139695598520192 basic_session_run_hooks.py:260] loss = 14.114265, step = 1170 (11.393 sec)\n",
            "I0623 20:26:35.895033 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.61468\n",
            "I0623 20:26:35.901407 139695598520192 basic_session_run_hooks.py:260] loss = 13.842731, step = 1200 (11.479 sec)\n",
            "I0623 20:26:48.173790 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.44324\n",
            "I0623 20:26:48.175131 139695598520192 basic_session_run_hooks.py:260] loss = 13.508487, step = 1230 (12.274 sec)\n",
            "I0623 20:27:00.141130 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.50682\n",
            "I0623 20:27:00.142949 139695598520192 basic_session_run_hooks.py:260] loss = 12.8764925, step = 1260 (11.968 sec)\n",
            "I0623 20:27:11.309240 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.68622\n",
            "I0623 20:27:11.310593 139695598520192 basic_session_run_hooks.py:260] loss = 13.856112, step = 1290 (11.168 sec)\n",
            "I0623 20:27:23.253411 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.51169\n",
            "I0623 20:27:23.254922 139695598520192 basic_session_run_hooks.py:260] loss = 13.6350565, step = 1320 (11.944 sec)\n",
            "I0623 20:27:34.462007 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.67653\n",
            "I0623 20:27:34.463474 139695598520192 basic_session_run_hooks.py:260] loss = 13.516556, step = 1350 (11.209 sec)\n",
            "I0623 20:27:46.203132 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.55512\n",
            "I0623 20:27:46.204458 139695598520192 basic_session_run_hooks.py:260] loss = 13.075546, step = 1380 (11.741 sec)\n",
            "I0623 20:27:57.788206 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.58953\n",
            "I0623 20:27:57.789470 139695598520192 basic_session_run_hooks.py:260] loss = 13.691109, step = 1410 (11.585 sec)\n",
            "I0623 20:28:09.721289 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.51402\n",
            "I0623 20:28:09.722715 139695598520192 basic_session_run_hooks.py:260] loss = 13.452864, step = 1440 (11.933 sec)\n",
            "I0623 20:28:21.713552 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.50161\n",
            "I0623 20:28:21.715600 139695598520192 basic_session_run_hooks.py:260] loss = 13.2515335, step = 1470 (11.993 sec)\n",
            "I0623 20:28:32.604963 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.75447\n",
            "I0623 20:28:32.610659 139695598520192 basic_session_run_hooks.py:260] loss = 13.957821, step = 1500 (10.895 sec)\n",
            "I0623 20:28:44.610175 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.49891\n",
            "I0623 20:28:44.611876 139695598520192 basic_session_run_hooks.py:260] loss = 13.705854, step = 1530 (12.001 sec)\n",
            "I0623 20:28:56.337638 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.5581\n",
            "I0623 20:28:56.338724 139695598520192 basic_session_run_hooks.py:260] loss = 13.322736, step = 1560 (11.727 sec)\n",
            "I0623 20:29:08.082686 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.55427\n",
            "I0623 20:29:08.084024 139695598520192 basic_session_run_hooks.py:260] loss = 13.6374035, step = 1590 (11.745 sec)\n",
            "I0623 20:29:20.521187 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.41187\n",
            "I0623 20:29:20.522647 139695598520192 basic_session_run_hooks.py:260] loss = 13.403533, step = 1620 (12.439 sec)\n",
            "I0623 20:29:32.073433 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.5969\n",
            "I0623 20:29:32.074900 139695598520192 basic_session_run_hooks.py:260] loss = 13.196542, step = 1650 (11.552 sec)\n",
            "I0623 20:29:43.917083 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.53301\n",
            "I0623 20:29:43.918528 139695598520192 basic_session_run_hooks.py:260] loss = 13.73217, step = 1680 (11.844 sec)\n",
            "I0623 20:29:56.028650 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.47696\n",
            "I0623 20:29:56.030644 139695598520192 basic_session_run_hooks.py:260] loss = 12.637638, step = 1710 (12.112 sec)\n",
            "I0623 20:30:08.814289 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.34638\n",
            "I0623 20:30:08.815429 139695598520192 basic_session_run_hooks.py:260] loss = 13.174345, step = 1740 (12.785 sec)\n",
            "I0623 20:30:20.314207 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.60879\n",
            "I0623 20:30:20.315665 139695598520192 basic_session_run_hooks.py:260] loss = 13.490089, step = 1770 (11.500 sec)\n",
            "I0623 20:30:31.940721 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.58024\n",
            "I0623 20:30:31.946696 139695598520192 basic_session_run_hooks.py:260] loss = 13.423707, step = 1800 (11.631 sec)\n",
            "I0623 20:30:42.621130 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.80888\n",
            "I0623 20:30:42.622655 139695598520192 basic_session_run_hooks.py:260] loss = 13.34984, step = 1830 (10.676 sec)\n",
            "I0623 20:30:54.488704 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.5279\n",
            "I0623 20:30:54.490095 139695598520192 basic_session_run_hooks.py:260] loss = 14.18818, step = 1860 (11.867 sec)\n",
            "I0623 20:31:06.243181 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.55221\n",
            "I0623 20:31:06.244591 139695598520192 basic_session_run_hooks.py:260] loss = 13.241049, step = 1890 (11.754 sec)\n",
            "I0623 20:31:18.548473 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.43798\n",
            "I0623 20:31:18.549790 139695598520192 basic_session_run_hooks.py:260] loss = 13.8484745, step = 1920 (12.305 sec)\n",
            "I0623 20:31:30.086874 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.60002\n",
            "I0623 20:31:30.088255 139695598520192 basic_session_run_hooks.py:260] loss = 13.014602, step = 1950 (11.538 sec)\n",
            "I0623 20:31:41.280421 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.68012\n",
            "I0623 20:31:41.281833 139695598520192 basic_session_run_hooks.py:260] loss = 13.4179, step = 1980 (11.194 sec)\n",
            "I0623 20:31:53.046540 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.54969\n",
            "I0623 20:31:53.047863 139695598520192 basic_session_run_hooks.py:260] loss = 13.601105, step = 2010 (11.766 sec)\n",
            "I0623 20:32:04.910520 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.52866\n",
            "I0623 20:32:04.912715 139695598520192 basic_session_run_hooks.py:260] loss = 13.942615, step = 2040 (11.865 sec)\n",
            "I0623 20:32:16.234517 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.64924\n",
            "I0623 20:32:16.235870 139695598520192 basic_session_run_hooks.py:260] loss = 14.02635, step = 2070 (11.323 sec)\n",
            "I0623 20:32:27.919708 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.56735\n",
            "I0623 20:32:27.924395 139695598520192 basic_session_run_hooks.py:260] loss = 12.571028, step = 2100 (11.688 sec)\n",
            "I0623 20:32:39.457928 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.60006\n",
            "I0623 20:32:39.459109 139695598520192 basic_session_run_hooks.py:260] loss = 13.366695, step = 2130 (11.535 sec)\n",
            "I0623 20:32:51.183454 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.55851\n",
            "I0623 20:32:51.184766 139695598520192 basic_session_run_hooks.py:260] loss = 13.898857, step = 2160 (11.726 sec)\n",
            "I0623 20:33:02.946110 139695598520192 basic_session_run_hooks.py:692] global_step/sec: 2.55044\n",
            "I0623 20:33:02.947383 139695598520192 basic_session_run_hooks.py:260] loss = 13.121043, step = 2190 (11.763 sec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_3TiPeZLPM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "#!tar -cvf best_model.tar best_model\n",
        "#files.download('best_model/model.ckpt-7100.data-00000-of-00001')\n",
        "\n",
        "#uploaded = drive.CreateFile({'title': 'best_model'})\n",
        "#uploaded.SetContentFile('/content/drive/My Drive/recsys/best_model.tar')\n",
        "#uploaded.Upload()\n",
        "#print('Uploaded file with ID {}'.format(uploaded.get('id')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nerihgMP46tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -r best_model\n",
        "#!rm -r /tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLKBnlIuscne",
        "colab_type": "text"
      },
      "source": [
        "https://stackoverflow.com/questions/47676248/accessing-validation-data-within-a-custom-callback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKHXotTr3zko",
        "colab_type": "text"
      },
      "source": [
        "~Use Keras [callbacks](https://github.com/keras-team/keras/issues/5794) instead of a custom metric?~ Outdated. Using tf.Estimator now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IxqaJL6tK4r",
        "colab_type": "text"
      },
      "source": [
        "## Generating predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThdH-XJAZZXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del train_split\n",
        "del val_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93vTjxkRtQ9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv(os.path.join(PATH, 'test.csv'))\n",
        "test = test[test['action_type'] == 'clickout item']\n",
        "test = test[test['reference'].isnull()]\n",
        "test['reference'].fillna(0, inplace=True)\n",
        "#test_all['reference'].fillna(0, inplace=True)\n",
        "# TODO: Use all of the test rows!!!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im7hl7qrIszn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputgen_test = InputGenerator(tokenizers,\n",
        "                  ['user_id', 'session_id', 'platform', 'device',\n",
        "                   'interacted_items', 'filters', 'items', 'timestamp'])\n",
        "\n",
        "same_keys = ['user_id', 'session_id', 'timestamp', 'step']\n",
        "\n",
        "def get_dataset_test():\n",
        "  gen_test, types, shapes = inputgen_test.input_generator_gen(test)\n",
        "  dataset = tf.data.Dataset.from_generator(gen_test, types).padded_batch(500, shapes)\n",
        "  batch_val = dataset.make_one_shot_iterator().get_next()\n",
        "  \n",
        "  return dataset\n",
        "\n",
        "inv_map = {(v - 1): k for k, v in item_tokenizer.mapping.items()}\n",
        "\n",
        "\n",
        "yt_rec_estimator = tf.estimator.Estimator(\n",
        "  model_fn=yt_rec_model_fn,\n",
        "  model_dir='./yt-7100',\n",
        "  params=params,\n",
        "  #config=run_config\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVo7zA6IXF3W",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HtUlDVrI8J3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_preds(filename, l_results):\n",
        "  r = pd.DataFrame(l_results)\n",
        "  r.to_csv(filename)\n",
        "  del r\n",
        "  uploaded = drive.CreateFile({'title': filename})\n",
        "  uploaded.SetContentFile(os.path.join('/content/drive/My Drive/recsys/',\n",
        "                                       filename))\n",
        "  uploaded.Upload()\n",
        "  print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEFRNgkIv2IX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_count = len(test)\n",
        "results = []\n",
        "predictions = yt_rec_estimator.predict(input_fn=get_dataset_test)\n",
        "test_iter = test.iterrows()\n",
        "counter = 0\n",
        "print('All rows: {}'.format(all_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqrdSWcWXe-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for (index, row), prediction in zip(test_iter, predictions):\n",
        "  result_row = {}\n",
        "  for r in same_keys:\n",
        "    result_row[r] = row[r]\n",
        "  \n",
        "  assert row['timestamp'] ==  prediction['timestamp']\n",
        "  impressions = list(map(int, row['impressions'].split('|')))\n",
        "  k = len(impressions)\n",
        "  items = item_tokenizer.transform(impressions)\n",
        "\n",
        "  mask = np.zeros((len(items), len(item_tokenizer.vocabulary)))\n",
        "  mask[np.arange(len(items)), items] = 1\n",
        "  mask = mask.sum(axis=0)\n",
        "  \n",
        "  scores = np.multiply(mask, prediction['probabilities'])\n",
        "  indices = scores.argsort()[-k:][::-1]\n",
        "\n",
        "  item_recommendations = []\n",
        "  for i in indices:\n",
        "    item_recommendations.append(inv_map[i])\n",
        "  \n",
        "  item_recommendations = map(str, item_recommendations)\n",
        "  result_row['item_recommendations'] = ' '.join(item_recommendations)\n",
        "  \n",
        "  results.append(result_row)\n",
        "  counter += 1\n",
        "\n",
        "  if counter % 1000 == 0:\n",
        "    print(counter/all_count)\n",
        "    print('PREDICTIONS: {}'.format(counter))\n",
        "    \n",
        "  if counter % 50_000 == 0:\n",
        "    print(counter/all_count)\n",
        "    print('PREDICTIONS: {}'.format(counter))\n",
        "    save_preds('results2/predictions-{}.csv'.format(counter), results)\n",
        "    del results\n",
        "    results = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg26UWLHIzCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_preds('results2/predictions-{}.csv'.format(counter), results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxe9M_cW5YPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}